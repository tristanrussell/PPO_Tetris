[DEFAULT]
checkpoint = 100000

[environment]
env_name = simpletetris
reward_step = True
penalise_height = False
penalise_height_increase = False
advanced_clears = False
high_scoring = False
penalise_holes = False
lock_delay = 0
observation = ram
extend_dims = True
render = False

[agent]
agent = PPO
steps_per_episode = 4096
gamma = 0.99
lambda = 0.97
clip_ratio = 0.2
entropy_beta = 0.01
critic_discount = 1.0
delta = 0.001
policy_learning_rate = 3e-4
value_function_learning_rate = 1e-3
train_policy_iterations = 20
train_value_iterations = 20
target_kl = 0.01
hidden_sizes = [512, 128]
hidden_sizes_activation = relu
conv_layers = [[32,4,[1,2]],[64,3,1]]
conv_layers_activation = relu
flatten = True

[criteria]
max_episodes = 100000
min_episodes_criterion = 50
reward_threshold = 10000
checkpoint_frequency = 50
gif_frequency = 100

