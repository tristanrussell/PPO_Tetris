[DEFAULT]
checkpoint = 0

[environment]
env_name = simpletetris
reward_step = True
penalise_height = False
penalise_height_increase = False
advanced_clears = False
high_scoring = False
penalise_holes = False
penalise_holes_increase = False
lock_delay = 0
step_reset = False
observation = ram
extend_dims = True
render = False

[agent]
agent = PPO
steps_per_episode = 4096
batch_size = 128
gamma = 0.99
lambda = 0.97
clip_ratio = 0.2
entropy_beta = 0.001
critic_discount = 1.0
; Delta is used only in TRGPPO
delta = 0.001
policy_learning_rate = 3e-4
value_function_learning_rate = 1e-3
train_policy_iterations = 20
train_value_iterations = 20
target_kl = 0.01
hidden_sizes = [512, 128]
hidden_sizes_activation = relu
conv_layers = [[32,4,[1,2]],[64,3,1]]
conv_layers_activation = relu
flatten = True

[criteria]
max_episodes = 50000
min_episodes_criterion = 50
reward_threshold = 1000
checkpoint_frequency = 50
gif_frequency = 100
