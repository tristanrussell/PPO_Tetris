[DEFAULT]
; The latest checkpoint, used for restarting tests
checkpoint = 0

[environment]
; The environment name, breakout | tetris | simpletetris
env_name = simpletetris
; Enable small rewards at every time step
reward_step = True
; Penalise the height of the tower
penalise_height = False
; Penalise the height increase of the tower
penalise_height_increase = False
; Change the scoring system to reward multiline clears higher
advanced_clears = False
; Increase the reward for clearing a line
high_scoring = False
; Penalise the number of holes in the tower
penalise_holes = False
; Penalise the increase in the number of holes in the tower
penalise_holes_increase = False
; The number of steps that the piece can move before locking when it hits a surface
lock_delay = 0
; Reset the lock delay timer if the piece continues falling
step_reset = False
; The observation mode, ram | grayscale | rgb
observation = ram
; Extend the dimensions from X x Y to X x Y x 1
extend_dims = True

[agent]
; The agent to use, PPO | TRGPPO
agent = PPO
; The number of steps per episode
steps_per_episode = 4096
; The minibatch size
batch_size = 128
; gamma parameter as used by PPO and TRGPPO
gamma = 0.99
; lambda parameter as used by PPO and TRGPPO
lambda = 0.97
; clip ratio (epsilon) parameter as used by PPO and TRGPPO
clip_ratio = 0.2
; entropy beta parameter as used by PPO and TRGPPO
entropy_beta = 0.001
; critic discount parameter as used by PPO and TRGPPO
critic_discount = 1.0
; delta discount parameter as used by TRGPPO
delta = 0.001
; actor (policy) learning rate
policy_learning_rate = 3e-4
; critic (value function) learning rate
value_function_learning_rate = 1e-3
; actor (policy) training iterations (epochs)
train_policy_iterations = 20
; critic (value function) training iterations (epochs)
train_value_iterations = 20
; target kl parameter as used by PPO and TRGPPO
target_kl = 0.01
; The number and size of the hidden fully connected layers in the neural network
hidden_sizes = [512, 128]
; The activation function for the fully connected layers in the neural network
hidden_sizes_activation = relu
; The number and size of the hidden convolution layers in the neural network
conv_layers = [[32,4,[1,2]],[64,3,1]]
; The activation function for the convolution layers in the neural network
conv_layers_activation = relu
; Flatten the input to the fully connected layers
flatten = True

[criteria]
; Maximum number of episodes
max_episodes = 50000
; Minimum number of episodes that must run before ending due to reaching the threshold
min_episodes_criterion = 50
; The reward threshold to finish at
reward_threshold = 10000
; The frequency of checkpoints
checkpoint_frequency = 50
; The frequency of gifs
gif_frequency = 100
